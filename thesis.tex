\documentclass[11pt,a4paper]{book}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Math symbols and environments
%
% These have to be imported before some later package to prevent some
% redefinition error.
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

% Imported by org-mode export
\usepackage{fixltx2e}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}

\usepackage{graphicx}
\usepackage[margin=3cm]{geometry}
\usepackage{microtype}
\usepackage[hidelinks]{hyperref}
\usepackage[style=alphabetic]{biblatex}
\usepackage{fancyhdr}

% Do not indent paragraphs
\usepackage{parskip}

\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}

\DeclareMathOperator{\supp}{supp}

\addbibresource{references.bib}

\begin{document}

\pagestyle{empty}

\frontmatter

% Hide page numbers in frontmatter
\pagenumbering{gobble}

\begin{titlepage}
  \begingroup
  \begin{center}
    \includegraphics[width=120pt]{thesis/logo}\\

    \vspace{9em}

    {\Huge Computations With Random Variables}\\
    \vspace{2em}
    {\large Bachelor's Thesis}\\
    \vfill
    \begin{minipage}{0.4\textwidth}
      \begin{flushleft}
        \emph{Author}\\
        \vspace{0.3em}
        {\Large Marten Lienen}
      \end{flushleft}
    \end{minipage}
    \hfill
    \begin{minipage}{0.4\textwidth}
      \begin{flushright}
        \emph{Supervised by}\\
        \vspace{0.3em}
        {\Large Prof. Dr. Stefan Harmeling}
      \end{flushright}
    \end{minipage}
    \\
    \vspace{6em}
    {\Large Heinrich-Heine-Universität Düsseldorf}\\
    \vspace{0.5em}
    {\large Computer Vision, Computer Graphics and Pattern Recognition}\\
    \vspace{1em}
    July 21, 2015
  \end{center}
  \endgroup
\end{titlepage}

\cleardoublepage

\tableofcontents

\cleardoublepage

\mainmatter
\pagestyle{fancy}

\chapter{Introduction}

\chapter{Our Approach}

\section{Closure of Mixtures}

Let $n \in \mathbb{N}_{0}$, $X$ a real random variable with mixture distribution
and $Y_{i}, i \in \{ 1, \dots, n \}$ another $n$ real random variables with
mixture distributions.
\begin{equation*}
  p(X = x) = \sum_{i = 1}^{m} \alpha_{i} \cdot p(X_{i} = x) \qquad p(Y_{i} = y) = \sum_{j = 1}^{m_{i}} \beta_{i,j} \cdot p(Y_{i,j} = y)
\end{equation*}

Consider a function $f : S \times_{i = 1}^{n} S_{i} \rightarrow \mathbb{R}$,
where $S \supseteq \supp(X)$ and
$S_{i} \supseteq \supp(Y_{i}), i \in \{ 1, \dots, n \}$. Then we might ask
ourselves, how $Z = f(X, Y_{1}, \dots, Y_{n})$ is distributed. So we are looking
to evaluate
\begin{equation*}
  p(Z = z) = \int_{S_{n}}\dots\int_{S_{1}} p(Z = 1, Y_{1} = y_{1}, \dots, Y_{n} = y_{n})~\mathrm{d}y_{1}\dots\mathrm{d}y_{n}
\end{equation*}

First, we have to tie the joint PDF of $Z$ and $Y_{1, \dots, n}$ to the joint
PDF of $X$ and $Y_{1, \dots, n}$. Such an expression can be derived using
\emph{multivariate change of variables} \cite[chapter~2.6.2.1]{murphy}, which I
will restate here.

\begin{lemma}[Multivariate change of variables]
  \label{lemma:multivariate-change}
  Let $f : \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$ be differentiable and
  invertible, $X \in \mathbb{R}^{n}$ a random vector and $Y = f(X)$. Then the
  distribution of $Y$ is given by
  \begin{equation*}
    p(Y = y) = \left| \det J \right| \cdot p(X = f^{-1}(y))
  \end{equation*}
  where $J$ is the Jacobian of $f^{-1}$, i.e.
  \begin{equation*}
    J = \frac{\partial X}{\partial Y} = \begin{pmatrix}
      \frac{\partial X_{1}}{\partial Y_{1}} & \cdots & \frac{\partial X_{1}}{\partial Y_{n}}\\
      \vdots & \ddots & \vdots\\
      \frac{\partial X_{n}}{\partial Y_{1}} & \cdots & \frac{\partial X_{n}}{\partial Y_{n}}
    \end{pmatrix}
  \end{equation*}
\end{lemma}

\begin{lemma}
  \label{lemma:joint-pdf}
  If $f$ is differentiable and invertible in the first argument, we can express
  the joint PDF of $Z$ and $Y_{1, \dots, n}$ in terms of the joint PDF of $X$
  and $Y_{1, \dots, n}$ as
  \begin{equation*}
    p(Z = z, Y_{1, \dots, n} = y_{1, \dots, n}) = \left| \frac{\mathrm{d}f_{y_{1}, \dots, y_{n}}^{-1}(z)}{\mathrm{d}z} \right| \cdot p\left(X = f_{y_{1}, \dots, y_{n}}^{-1}(z), Y_{1, \dots, n} = y_{1, \dots, n}\right)
  \end{equation*}
\end{lemma}
\begin{proof}
  Define $g : S \times_{i = 1}^{n} S_{i} \rightarrow \mathbb{R}^{n + 1}$ as
  \begin{equation*}
    g \begin{pmatrix}
      x\\y_{1}\\\vdots\\y_{n}
    \end{pmatrix} = \begin{pmatrix}
      f(x, y_{1}, \dots, y_{n})\\
      y_{1}\\\vdots\\y_{n}
    \end{pmatrix}
  \end{equation*}
  Then $g$ is invertible and differentiable and the inverse is given by
  \begin{equation*}
    g^{-1}\begin{pmatrix}
      z\\y_{1}\\\vdots\\y_{n}
    \end{pmatrix} = \begin{pmatrix}
      f_{y_{1}, \dots, y_{n}}^{-1}(z)\\
      y_{1}\\\vdots\\y_{n}
    \end{pmatrix}
  \end{equation*}
  Next we examine the Jacobian $J$ of $g^{-1}$. Let
  $(z, y_{1}, \dots, y_{n})^{T} \in \mathbb{R}^{n + 1}$.
  \begin{equation*}
    J :=
    \begin{pmatrix}
      \frac{\mathrm{d}f_{y_{1}, \dots, y_{n}}^{-1}}{\partial z} & \frac{\mathrm{d}f_{y_{1}, \dots, y_{n}}^{-1}}{\partial y_{1}} & \cdots & \frac{\mathrm{d}f_{y_{1}, \dots, y_{n}}^{-1}}{\partial y_{n}}\\
      \frac{\mathrm{d} y_{1}}{\mathrm{d} z} & \frac{\mathrm{d} y_{1}}{\mathrm{d} y_{1}} & \cdots & \frac{\mathrm{d} y_{1}}{\mathrm{d} y_{n}}\\
      \vdots & \vdots & \ddots & \vdots\\
      \frac{\mathrm{d} y_{n}}{\mathrm{d} z} & \frac{\mathrm{d} y_{n}}{\mathrm{d} y_{1}} & \cdots & \frac{\mathrm{d} y_{n}}{\mathrm{d} y_{n}}
    \end{pmatrix}
    =
    \begin{pmatrix}
      \frac{\mathrm{d}f_{y_{1}, \dots, y_{n}}^{-1}}{\partial z} & \frac{\mathrm{d}f_{y_{1}, \dots, y_{n}}^{-1}}{\partial y_{1}} & \cdots & \frac{\mathrm{d}f_{y_{1}, \dots, y_{n}}^{-1}}{\partial y_{n}}\\
      0 & 1 & \cdots & 0\\
      \vdots & \vdots & \ddots & \vdots\\
      0 & 0 & \cdots & 1
    \end{pmatrix}
  \end{equation*}
  $J$ is an upper-triangular matrix, which means
  \begin{equation*}
    \det J = \frac{\mathrm{d}f_{y_{1}, \dots, y_{n}}^{-1}}{\partial z}
  \end{equation*}
  Therefore the claimed equality follows from lemma \ref{lemma:multivariate-change}.
\end{proof}

Equipped with lemma \ref{lemma:joint-pdf} we can now derive the general form of
$Z$'s distribution.

\begin{lemma}
  If $f$ is differentiable and invertible in the first argument, the
  distribution of $Z$ is a mixture distribution given by the following equation
  \begin{equation*}
    p(Z = z) = \sum_{i = 1}^{m} \sum_{k_{1} = 1}^{m_{1}} \dots \sum_{k_{n} = 1}^{m_{n}} \alpha_{i} \left( \prod_{j = 1}^{n} \beta_{j,k_{j}} \right) p(f(X_{i}, Y_{1,k_{1}}, \dots, Y_{n,k_{n}}) = z)
  \end{equation*}
\end{lemma}
\begin{proof}
  Let $x = f_{y_{1}, \dots, y_{n}}^{-1}(z)$.
  \begin{align*}
    p(Z = z) & = \int_{S_{n}} \dots \int_{S_{1}} p(Z = z, Y_{1, \dots, n} = y_{1, \dots, n})~\mathrm{d}y_{1}\dots \mathrm{d}y_{n}\\
             & = \int_{S_{n}} \dots \int_{S_{1}} \left| \frac{\mathrm{d}x}{\mathrm{d}z} \right| \cdot p\left(X = f_{y_{1}, \dots, y_{n}}^{-1}(z), Y_{1, \dots, n} = y_{1, \dots, n}\right)~\mathrm{d}y_{1}\dots \mathrm{d}y_{n}\\
             & = \int_{S_{n}} \dots \int_{S_{1}} \left| \frac{\mathrm{d}x}{\mathrm{d}z} \right| \cdot p\left(X = f_{y_{1}, \dots, y_{n}}^{-1}(z)\right) \prod_{j = 1}^{n} p\left(Y_{j} = y_{j}\right)~\mathrm{d}y_{1}\dots \mathrm{d}y_{n}\\
             & = \int_{S_{n}} \dots \int_{S_{1}} \left| \frac{\mathrm{d}x}{\mathrm{d}z} \right| \cdot \left(\sum_{i = 1}^{m} \alpha_{i} \cdot p(X_{i} = x)\right) \prod_{j = 1}^{n} \left(\sum_{k = 1}^{m_{j}} \beta_{j,k} \cdot p(Y_{j,k} = y_{j})\right)~\mathrm{d}y_{1}\dots \mathrm{d}y_{n}\\
             & = \sum_{i = 1}^{m} \sum_{k_{1} = 1}^{m_{1}} \dots \sum_{k_{n} = 1}^{m_{n}} \alpha_{i} \left( \prod_{j = 1}^{n} \beta_{j,k_{j}} \right) \int_{S_{n}} \dots \int_{S_{1}} \left| \frac{\mathrm{d}x}{\mathrm{d}z} \right| \cdot p(X_{i} = x) \cdot \prod_{j = 1}^{n} p(Y_{j,k_{j}} = y_{j})~\mathrm{d}y_{1}\dots \mathrm{d}y_{n}\\
             & = \sum_{i = 1}^{m} \sum_{k_{1} = 1}^{m_{1}} \dots \sum_{k_{n} = 1}^{m_{n}} \alpha_{i} \left( \prod_{j = 1}^{n} \beta_{j,k_{j}} \right) \int_{S_{n}} \dots \int_{S_{1}} \left| \frac{\mathrm{d}x}{\mathrm{d}z} \right| \cdot p(X_{i} = x, Y_{1,k_{1}} = y_{1}, \dots, Y_{n,k_{n}} = y_{n})~\mathrm{d}y_{1}\dots \mathrm{d}y_{n}\\
             & = \sum_{i = 1}^{m} \sum_{k_{1} = 1}^{m_{1}} \dots \sum_{k_{n} = 1}^{m_{n}} \alpha_{i} \left( \prod_{j = 1}^{n} \beta_{j,k_{j}} \right) \int_{S_{n}} \dots \int_{S_{1}} p(f(X_{i}, Y_{1,k_{1}}, \dots, Y_{n,k_{n}}) = z, Y_{1,k_{1}} = y_{1}, \dots, Y_{n,k_{n}} = y_{n})~\mathrm{d}y_{1}\dots \mathrm{d}y_{n}\\
             & = \sum_{i = 1}^{m} \sum_{k_{1} = 1}^{m_{1}} \dots \sum_{k_{n} = 1}^{m_{n}} \alpha_{i} \left( \prod_{j = 1}^{n} \beta_{j,k_{j}} \right) p(f(X_{i}, Y_{1,k_{1}}, \dots, Y_{n,k_{n}}) = z)
  \end{align*}
\end{proof}

\section{Monadic Functions}

\section{Diadic Functions}

\section{Scalars and Random Variables}

Let $X$ be a random variable with a gaussian mixture distribution.
\begin{equation*}
  X \sim \sum_{i = 1}^{n} \alpha_{i} \cdot \mathcal{N}(\mu_{i}, \sigma_{i}^{2})
\end{equation*}

\begin{lemma}
  \begin{equation*}
    -X \sim \sum_{i = 1}^{n} \alpha_{i} \cdot \mathcal{N}(-\mu_{i}, \sigma_{i}^{2})
  \end{equation*}
\end{lemma}

\begin{proof}
  Trivial.
\end{proof}

\begin{lemma}
  Let $f \in \mathbb{R}$.
  \begin{equation*}
    f + X = X + f \sim \sum_{i = 1}^{n} \alpha_{i} \cdot \mathcal{N}(\mu_{i} + f, \sigma_{i}^{2})
  \end{equation*}
\end{lemma}

\begin{proof}
  \begin{align*}
    p(X + f = x) & = p(X = x - f) \cdot \left| \frac{\mathrm{d}(x + f)}{\mathrm{d}x} \right| = p(X = x - f)\\
                 & = \sum_{i = 1}^{n} \alpha_{i} \cdot \mathcal{N}(x - f \mid \mu_{i}, \sigma_{i}^{2})\\
                 & = \sum_{i = 1}^{n} \alpha_{i} \cdot \mathcal{N}(x \mid \mu_{i} + f, \sigma_{i}^{2})
  \end{align*}
\end{proof}

\begin{corollary}
  Let $f \in \mathbb{R}$.
  \begin{equation*}
    X - f \sim \sum_{i = 1}^{n} \alpha_{i} \cdot \mathcal{N}(\mu_{i} - f, \sigma_{i}^{2})
  \end{equation*}
\end{corollary}

\begin{proof}
  $X - f = X + (-f)$
\end{proof}

\begin{corollary}
  Let $f \in \mathbb{R}$.
  \begin{equation*}
    f - X \sim \sum_{i = 1}^{n} \alpha_{i} \cdot \mathcal{N}(f - \mu_{i}, \sigma_{i}^{2})
  \end{equation*}
\end{corollary}

\begin{proof}
  $f - X = f + (-X)$
\end{proof}

\begin{lemma}
  Let $f \in \mathbb{R} \setminus \{ 0 \}$.
  \begin{equation*}
    fX = Xf \sim \sum_{i = 1}^{n} \alpha_{i} \cdot \mathcal{N}(f\mu_{i}, f^{2}\sigma_{i}^{2})
  \end{equation*}
\end{lemma}

\begin{proof}
  \begin{align*}
    p(fX = x) & = \left| \frac{\mathrm{d}\frac{x}{f}}{\mathrm{d}x} \right| \cdot p\left( X = \frac{x}{f} \right)\\
              & = \left|\frac{1}{f}\right| \cdot p\left( X = \frac{x}{f} \right)\\
              & = \left|\frac{1}{f}\right| \cdot \sum_{i = 1}^{n} \alpha_{i} \cdot \mathcal{N}\left(\frac{x}{f} \mid \mu_{i}, \sigma_{i}^{2}\right)\\
              & = \left|\frac{1}{f}\right| \cdot \sum_{i = 1}^{n} \alpha_{i} \cdot |f| \cdot \mathcal{N}\left(x \mid f\mu_{i}, f^{2}\sigma_{i}^{2}\right)\\
              & = \sum_{i = 1}^{n} \alpha_{i} \cdot \mathcal{N}\left(x \mid f\mu_{i}, f^{2}\sigma_{i}^{2}\right)
  \end{align*}
\end{proof}

\begin{corollary}
  Let $f \in \mathbb{R} \setminus \{ 0 \}$.
  \begin{equation*}
    \frac{X}{f} \sim \sum_{i = 1}^{n} \alpha_{i} \cdot \mathcal{N}\left( \frac{\mu_{i}}{f}, \frac{\sigma_{i}^{2}}{f^{2}} \right)
  \end{equation*}
\end{corollary}

\begin{proof}
  $\frac{X}{f} = X \cdot \frac{1}{f}$
\end{proof}

\section{Basic Arithmethic}

However with a general $f$ we cannot determine the component distributions. But
in specific cases this might be possible, as we see in the next example.

\begin{example}
  If $f = +$ or $f = -$, $f$ is monotonic and invertible in the first
  argument. Also $f$ of two normally distributed random variables is again
  normally distributed, so $Z$ is another mixture of gaussians with components
  \begin{equation*}
    Z_{i,j} \sim \mathcal{N}(\mu_{i} + \nu_{j}, \sigma_{i}^{2} + \tau_{j}^{2})
  \end{equation*}
\end{example}

\begin{lemma}
  If $f(X_{i}, Y_{j})$ has a gaussian mixture distribution with $o_{i,j}$
  components, $Z$ has a gaussian mixture distribution with
  $\sum_{i = 1}^{n} \sum_{j = 1}^{m} o_{i,j}$ components.
\end{lemma}

\begin{proof}
  Trivial.
\end{proof}

\section{Multiplication}

Multiplication is indeed monotonic and invertible in the first argument, but
unfortunately gaussian distributions are not closed under it. Actually the
product of two gaussians has a two-sided chi-squared distribution. So to stay
inside the realm of mixtures of gaussians, we have to approximate it with yet
another mixture of gaussians.

Let $X$ and $Y$ be two normally distributed variables
\begin{equation*}
  X \sim \mathcal{N}(\mu, \sigma^{2}) \qquad Y \sim \mathcal{N}(\nu, \tau^{2})
\end{equation*}
and $Z = X \cdot Y$ their product. Then $Z$ is distributed as follows
\begin{align*}
  p(Z = z) & = \int_{-\infty}^{\infty} \frac{1}{|y|} \cdot p\left(X = \frac{z}{y}\right) \cdot p(Y = y)~\mathrm{d}y\\
           & = \int_{-\infty}^{\infty} \frac{1}{|y|} \cdot \mathcal{N}\left( \frac{z}{y} \mid \mu, \sigma^{2} \right) \cdot \mathcal{N}(y \mid \nu, \tau^{2})~\mathrm{d}y\\
           & = \int_{-\infty}^{\infty} \frac{1}{|y|} \cdot |y| \cdot \mathcal{N}\left( z \mid y\mu, y^{2}\sigma^{2} \right) \cdot \mathcal{N}(y \mid \nu, \tau^{2})~\mathrm{d}y\\
           & = \int_{-\infty}^{\infty} \mathcal{N}\left( z \mid y\mu, y^{2}\sigma^{2} \right) \cdot \mathcal{N}(y \mid \nu, \tau^{2})~\mathrm{d}y
\end{align*}

We would like to approximate this with Gauss-Hermite quadrature. Therefore we
have to transform the integrand into the form $f(x) \cdot \exp(-x^{2})$ with the
following substitution.
\begin{equation*}
  x = \frac{y - \nu}{\sqrt{2}\tau} \Leftrightarrow y = \sqrt{2}\tau x + \nu \qquad \frac{\mathrm{d}x}{\mathrm{d}y} = \frac{1}{\sqrt{2}\tau}
\end{equation*}
\begin{align*}
  p(Z = z) & = \int_{-\infty}^{\infty} \mathcal{N}\left( z \mid y\mu, y^{2}\sigma^{2} \right) \cdot \frac{1}{\sqrt{2\pi\tau^{2}}} \exp\left( -\frac{(y - \nu)^{2}}{2\tau^{2}} \right)~\mathrm{d}y\\
           & = \frac{1}{\sqrt{\pi}} \int_{-\infty}^{\infty} \mathcal{N}\left( z \mid \left(\sqrt{2}\tau x + \nu\right)\mu, \left(\sqrt{2}\tau x + \nu\right)^{2}\sigma^{2} \right) \cdot \exp\left( -x^{2} \right)~\mathrm{d}x
\end{align*}

Now that we have achieved the right form, we can apply the approximation.
\begin{equation*}
  p(Z = z) \approx \sum_{i = 1}^{n} \frac{w_{i}}{\sqrt{\pi}} \cdot \mathcal{N}\left( z \mid \left(\sqrt{2 \tau^{2}} x_{i} + \nu\right)\mu, \left(\sqrt{2 \tau^{2}} x_{i} + \nu\right)^{2}\sigma^{2} \right)
\end{equation*}
where $x_{i}$ and $w_{i}$ are the positions and weights as defined by the
Gauss-Hermite quadrature method.

So we have approximated the product as a mixture of gaussians. Curiously the
weights even sum to $1$.

\section{Division}

Let $X$ and $Y$ be two normally distributed variables
\begin{equation*}
  X \sim \mathcal{N}(\mu, \sigma^{2}) \qquad Y \sim \mathcal{N}(\nu, \tau^{2})
\end{equation*}
and $Z = \frac{X}{Y}$ their quotient. Then $Z$ is distributed as follows
\begin{align*}
  p(Z = z) & = \int_{-\infty}^{\infty} |y| \cdot p\left(X = zy\right) \cdot p(Y = y)~\mathrm{d}y\\
           & = \int_{-\infty}^{\infty} |y| \cdot \mathcal{N}\left( zy \mid \mu, \sigma^{2} \right) \cdot \mathcal{N}(y \mid \nu, \tau^{2})~\mathrm{d}y\\
           & = \int_{-\infty}^{\infty} |y| \cdot \frac{1}{|y|} \cdot \mathcal{N}\left( z \mid \frac{\mu}{y}, \frac{\sigma^{2}}{y^{2}} \right) \cdot \mathcal{N}(y \mid \nu, \tau^{2})~\mathrm{d}y\\
           & = \int_{-\infty}^{\infty} \mathcal{N}\left( z \mid \frac{\mu}{y}, \frac{\sigma^{2}}{y^{2}} \right) \cdot \mathcal{N}(y \mid \nu, \tau^{2})~\mathrm{d}y
\end{align*}
Now we can apply the same substitution as for multiplication to bring it into
Gauss-Hermite quadrature form and get
\begin{equation*}
  p(Z = z) \approx \sum_{i = 1}^{n} \frac{w_{i}}{\sqrt{\pi}} \cdot \mathcal{N}\left( z \mid \frac{\mu}{\sqrt{2 \tau^{2}} x_{i} + \nu}, \frac{\sigma^{2}}{\left(\sqrt{2 \tau^{2}} x_{i} + \nu\right)^{2}} \right)
\end{equation*}

\printbibliography

\end{document}