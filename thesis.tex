\documentclass[11pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{amssymb}
\usepackage[margin=3cm]{geometry}
\usepackage{microtype}
\usepackage[hidelinks]{hyperref}
\usepackage[style=alphabetic]{biblatex}

\usepackage{amsthm}

\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}

\addbibresource{references.bib}

\author{Marten Lienen}
\date{July 21, 2015}
\title{Computations with random variables}

\begin{document}

\maketitle

\tableofcontents

\chapter{Introduction}

\chapter{Our Approach}


\section{Scalars and Random Variables}

Let $X$ be a random variable with a gaussian mixture distribution.
\begin{equation*}
  X \sim \sum_{i = 1}^{n} \alpha_{i} \cdot \mathcal{N}(\mu_{i}, \sigma_{i}^{2})
\end{equation*}

\begin{lemma}
  \begin{equation*}
    -X \sim \sum_{i = 1}^{n} \alpha_{i} \cdot \mathcal{N}(-\mu_{i}, \sigma_{i}^{2})
  \end{equation*}
\end{lemma}

\begin{proof}
  Trivial.
\end{proof}

\begin{lemma}
  Let $f \in \mathbb{R}$.
  \begin{equation*}
    f + X = X + f \sim \sum_{i = 1}^{n} \alpha_{i} \cdot \mathcal{N}(\mu_{i} + f, \sigma_{i}^{2})
  \end{equation*}
\end{lemma}

\begin{proof}
  \begin{align*}
    p(X + f = x) & = p(X = x - f) \cdot \left| \frac{\mathrm{d}(x + f)}{\mathrm{d}x} \right| = p(X = x - f)\\
                 & = \sum_{i = 1}^{n} \alpha_{i} \cdot \mathcal{N}(x - f \mid \mu_{i}, \sigma_{i}^{2})\\
                 & = \sum_{i = 1}^{n} \alpha_{i} \cdot \mathcal{N}(x \mid \mu_{i} + f, \sigma_{i}^{2})
  \end{align*}
\end{proof}

\begin{corollary}
  Let $f \in \mathbb{R}$.
  \begin{equation*}
    X - f \sim \sum_{i = 1}^{n} \alpha_{i} \cdot \mathcal{N}(\mu_{i} - f, \sigma_{i}^{2})
  \end{equation*}
\end{corollary}

\begin{proof}
  $X - f = X + (-f)$
\end{proof}

\begin{corollary}
  Let $f \in \mathbb{R}$.
  \begin{equation*}
    f - X \sim \sum_{i = 1}^{n} \alpha_{i} \cdot \mathcal{N}(f - \mu_{i}, \sigma_{i}^{2})
  \end{equation*}
\end{corollary}

\begin{proof}
  $f - X = f + (-X)$
\end{proof}

\begin{lemma}
  Let $f \in \mathbb{R} \setminus \{ 0 \}$.
  \begin{equation*}
    fX = Xf \sim \sum_{i = 1}^{n} \alpha_{i} \cdot \mathcal{N}(f\mu_{i}, f^{2}\sigma_{i}^{2})
  \end{equation*}
\end{lemma}

\begin{proof}
  \begin{align*}
    p(fX = x) & = \left| \frac{\mathrm{d}\frac{x}{f}}{\mathrm{d}x} \right| \cdot p\left( X = \frac{x}{f} \right)\\
              & = \left|\frac{1}{f}\right| \cdot p\left( X = \frac{x}{f} \right)\\
              & = \left|\frac{1}{f}\right| \cdot \sum_{i = 1}^{n} \alpha_{i} \cdot \mathcal{N}\left(\frac{x}{f} \mid \mu_{i}, \sigma_{i}^{2}\right)\\
              & = \left|\frac{1}{f}\right| \cdot \sum_{i = 1}^{n} \alpha_{i} \cdot |f| \cdot \mathcal{N}\left(x \mid f\mu_{i}, f^{2}\sigma_{i}^{2}\right)\\
              & = \sum_{i = 1}^{n} \alpha_{i} \cdot \mathcal{N}\left(x \mid f\mu_{i}, f^{2}\sigma_{i}^{2}\right)
  \end{align*}
\end{proof}

\begin{corollary}
  Let $f \in \mathbb{R} \setminus \{ 0 \}$.
  \begin{equation*}
    \frac{X}{f} \sim \sum_{i = 1}^{n} \alpha_{i} \cdot \mathcal{N}\left( \frac{\mu_{i}}{f}, \frac{\sigma_{i}^{2}}{f^{2}} \right)
  \end{equation*}
\end{corollary}

\begin{proof}
  $\frac{X}{f} = X \cdot \frac{1}{f}$
\end{proof}

\section{Basic Arithmethic}

Let $X, Y$ be two independent random variables with gaussian mixture
distributions.
\begin{equation*}
  X \sim \sum_{i = 1}^{n} \alpha_{i} \cdot \mathcal{N}(\mu_{i}, \sigma_{i}^{2}) \qquad Y \sim \sum_{i = 1}^{m} \beta_{i} \cdot \mathcal{N}(\nu_{i}, \tau_{i}^{2})
\end{equation*}
For future reference we will also define random variables for the components.
\begin{equation*}
  X_{i} \sim \mathcal{N}(\mu_{i}, \sigma_{i}^{2}) \qquad Y_{i} \sim \mathcal{N}(\nu_{i}, \tau_{i}^{2})
\end{equation*}

Let $f : \mathbb{R} \times \mathbb{R} \mapsto \mathbb{R}$ be monotonic and
invertible in the first argument and $Z = f(X, Y)$ the application of $f$ to $X$
and $Y$.

\begin{lemma}
  The joint PDF of $Z$ and $Y$ is expressible in terms of the joint PDF of $X$
  and $Y$.
\end{lemma}
\begin{proof}
  Let $x = f^{-1}(y, z)$.
  \begin{align*}
    p(Z = z, Y = y) & = p(Z = z \mid Y = y) \cdot p(Y = y)\\
                    & = \left| \frac{\mathrm{d}x}{\mathrm{d}z} \right| \cdot p(X = f^{-1}(y, z) \mid Y = y) \cdot p(Y = y) \quad\textit{Change of Variables, Murphy 2.6.2}\\
                    & = \left| \frac{\mathrm{d}x}{\mathrm{d}z} \right| \cdot p(X = f^{-1}(y, z)) \cdot p(Y = y) \quad\textit{$X$ and $Y$ are independent}\\
                    & = \left| \frac{\mathrm{d}x}{\mathrm{d}z} \right| \cdot p(X = f^{-1}(y, z), Y = y)
  \end{align*}
\end{proof}

Using this result we can infer the general form of $Z$.

\begin{lemma}
  $Z$ has a mixture distribution with $n \cdot m$ components.
  \begin{equation*}
    p(Z = z) = \sum_{i = 1}^{n} \sum_{j = 1}^{m} \alpha_{i}\beta_{j} \cdot p(f(X_{i}, Y_{j}) = z)
  \end{equation*}
\end{lemma}
\begin{proof}
  \begin{align*}
    p(Z = z) & = \int_{-\infty}^{\infty} p(Z = z, Y = y)~\mathrm{d}y\\
             & = \int_{-\infty}^{\infty} \left| \frac{\mathrm{d}x}{\mathrm{d}z} \right| \cdot p(X = f^{-1}(y, z)) \cdot p(Y = y)~\mathrm{d}y\\
             & = \int_{-\infty}^{\infty} \left| \frac{\mathrm{d}x}{\mathrm{d}z} \right| \cdot \left( \sum_{i = 1}^{n} \alpha_{i} \cdot \mathcal{N}(f^{-1}(y, z) \mid \mu_{i}, \sigma_{i}^{2}) \right) \cdot \left( \sum_{j = 1}^{m} \beta_{j} \cdot \mathcal{N}(y \mid \nu_{j}, \tau_{j}^{2}) \right)~\mathrm{d}y\\
             & = \sum_{i = 1}^{n} \sum_{j = 1}^{m} \alpha_{i}\beta_{j} \int_{-\infty}^{\infty} \left| \frac{\mathrm{d}x}{\mathrm{d}z} \right| \cdot \mathcal{N}(f^{-1}(y, z) \mid \mu_{i}, \sigma_{i}^{2}) \cdot \mathcal{N}(y \mid \nu_{j}, \tau_{j}^{2})~\mathrm{d}y\\
             & = \sum_{i = 1}^{n} \sum_{j = 1}^{m} \alpha_{i}\beta_{j} \int_{-\infty}^{\infty} \left| \frac{\mathrm{d}x}{\mathrm{d}z} \right| \cdot p(X_{i} = f^{-1}(y, z)) \cdot p(Y_{j} = y)~\mathrm{d}y\\
             & = \sum_{i = 1}^{n} \sum_{j = 1}^{m} \alpha_{i}\beta_{j} \int_{-\infty}^{\infty} p(f(X_{i}, y) = z, Y_{j} = y)~\mathrm{d}y\\
             & = \sum_{i = 1}^{n} \sum_{j = 1}^{m} \alpha_{i}\beta_{j} \cdot p(f(X_{i}, Y_{j}) = z)
  \end{align*}
\end{proof}

However with a general $f$ we cannot determine the component distributions. But
in specific cases this might be possible, as we see in the next example.

\begin{example}
  If $f = +$ or $f = -$, $f$ is monotonic and invertible in the first
  argument. Also $f$ of two normally distributed random variables is again
  normally distributed, so $Z$ is another mixture of gaussians with components
  \begin{equation*}
    Z_{i,j} \sim \mathcal{N}(\mu_{i} + \nu_{j}, \sigma_{i}^{2} + \tau_{j}^{2})
  \end{equation*}
\end{example}

\begin{lemma}
  If $f(X_{i}, Y_{j})$ has a gaussian mixture distribution with $o_{i,j}$
  components, $Z$ has a gaussian mixture distribution with
  $\sum_{i = 1}^{n} \sum_{j = 1}^{m} o_{i,j}$ components.
\end{lemma}

\begin{proof}
  Trivial.
\end{proof}

\section{Multiplication}

Multiplication is indeed monotonic and invertible in the first argument, but
unfortunately gaussian distributions are not closed under it. Actually the
product of two gaussians has a two-sided chi-squared distribution. So to stay
inside the realm of mixtures of gaussians, we have to approximate it with yet
another mixture of gaussians.

Let $X$ and $Y$ be two normally distributed variables
\begin{equation*}
  X \sim \mathcal{N}(\mu, \sigma^{2}) \qquad Y \sim \mathcal{N}(\nu, \tau^{2})
\end{equation*}
and $Z = X \cdot Y$ their product. Then $Z$ is distributed as follows
\begin{align*}
  p(Z = z) & = \int_{-\infty}^{\infty} \frac{1}{|y|} \cdot p\left(X = \frac{z}{y}\right) \cdot p(Y = y)~\mathrm{d}y\\
           & = \int_{-\infty}^{\infty} \frac{1}{|y|} \cdot \mathcal{N}\left( \frac{z}{y} \mid \mu, \sigma^{2} \right) \cdot \mathcal{N}(y \mid \nu, \tau^{2})~\mathrm{d}y\\
           & = \int_{-\infty}^{\infty} \frac{1}{|y|} \cdot |y| \cdot \mathcal{N}\left( z \mid y\mu, y^{2}\sigma^{2} \right) \cdot \mathcal{N}(y \mid \nu, \tau^{2})~\mathrm{d}y\\
           & = \int_{-\infty}^{\infty} \mathcal{N}\left( z \mid y\mu, y^{2}\sigma^{2} \right) \cdot \mathcal{N}(y \mid \nu, \tau^{2})~\mathrm{d}y
\end{align*}

We would like to approximate this with Gauss-Hermite quadrature. Therefore we
have to transform the integrand into the form $f(x) \cdot \exp(-x^{2})$ with the
following substitution.
\begin{equation*}
  x = \frac{y - \nu}{\sqrt{2}\tau} \Leftrightarrow y = \sqrt{2}\tau x + \nu \qquad \frac{\mathrm{d}x}{\mathrm{d}y} = \frac{1}{\sqrt{2}\tau}
\end{equation*}
\begin{align*}
  p(Z = z) & = \int_{-\infty}^{\infty} \mathcal{N}\left( z \mid y\mu, y^{2}\sigma^{2} \right) \cdot \frac{1}{\sqrt{2\pi\tau^{2}}} \exp\left( -\frac{(y - \nu)^{2}}{2\tau^{2}} \right)~\mathrm{d}y\\
           & = \frac{1}{\sqrt{\pi}} \int_{-\infty}^{\infty} \mathcal{N}\left( z \mid \left(\sqrt{2}\tau x + \nu\right)\mu, \left(\sqrt{2}\tau x + \nu\right)^{2}\sigma^{2} \right) \cdot \exp\left( -x^{2} \right)~\mathrm{d}x
\end{align*}

Now that we have achieved the right form, we can apply the approximation.
\begin{equation*}
  p(Z = z) \approx \sum_{i = 1}^{n} \frac{w_{i}}{\sqrt{\pi}} \cdot \mathcal{N}\left( z \mid \left(\sqrt{2 \tau^{2}} x_{i} + \nu\right)\mu, \left(\sqrt{2 \tau^{2}} x_{i} + \nu\right)^{2}\sigma^{2} \right)
\end{equation*}
where $x_{i}$ and $w_{i}$ are the positions and weights as defined by the
Gauss-Hermite quadrature method.

So we have approximated the product as a mixture of gaussians. Curiously the
weights even sum to $1$.

\chapter{References}

\printbibliography

\end{document}