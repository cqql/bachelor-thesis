\documentclass[11pt,a4paper]{book}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Math symbols and environments
%
% These have to be imported before some later package to prevent some
% redefinition error.
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

% Useful extensions for math
\usepackage{mathtools}

% Redefine spacing for proofs
\usepackage{thmtools}
\declaretheoremstyle[
  spaceabove=-4pt,
  spacebelow=8pt,
  headfont=\normalfont\itshape,
  postheadspace=1em,
  qed=\qedsymbol
]{nospacing}
\declaretheorem[name={Proof},style=nospacing,unnumbered]{proof2}

\usepackage{graphicx}
\usepackage[margin=3cm]{geometry}
\usepackage{microtype}
\usepackage[hidelinks]{hyperref}
\usepackage[style=alphabetic]{biblatex}
\usepackage{fancyhdr}

% Define subfigures
\usepackage{subcaption}

% Define an appendix
\usepackage[titletoc]{appendix}

% Do not indent paragraphs
\usepackage{parskip}

% Syntax highlighting
\usepackage{minted}

% Set global minted options
\setminted{linenos, autogobble, frame=lines, framesep=2mm}

% Do not show red boxes around characters, that pygments did not recognize,
% e.g. greek symbols
\makeatletter
\expandafter\def\csname PYGdefault@tok@err\endcsname{\def\PYGdefault@bc##1{{\strut ##1}}}
\makeatother

% Define replacements for UTF-8 characters in source code
\usepackage{newunicodechar}
\newunicodechar{μ}{$\mu$}
\newunicodechar{Σ}{$\Sigma$}

\newtheorem{lemma}{Lemma}
\newtheorem*{corollary}{Corollary}

\DeclareMathOperator{\supp}{supp}

\newcommand{\ontopof}[2]{
  \begin{array}{c}
    #1,\\#2
  \end{array}
}

\addbibresource{references.bib}

\begin{document}

\pagestyle{empty}

\frontmatter

% Hide page numbers in frontmatter
\pagenumbering{gobble}

\begin{titlepage}
  \begingroup
  \begin{center}
    \includegraphics[width=120pt]{thesis/logo}\\

    \vspace{9em}

    {\Huge Computations with Arbitrarily Distributed Random Variables}\\
    \vspace{2em}
    {\large Bachelor's Thesis}\\
    \vfill
    \small{\emph{Author}}\\
    \vspace{0.3em}
    {\LARGE Marten Lienen}\\
    \vspace{6em}
    \begin{minipage}{0.4\textwidth}
      \begin{flushleft}
        \small{\emph{Supervised by}}\\
        \vspace{0.3em}
        {\large Prof. Dr. Stefan Harmeling}
      \end{flushleft}
    \end{minipage}
    \hfill
    \begin{minipage}{0.4\textwidth}
      \begin{flushright}
        \small{\emph{Co-reviewed by}}\\
        \vspace{0.3em}
        {\large Prof. Dr. Michael Leuschel}
      \end{flushright}
    \end{minipage}
    \\
    \vspace{4em}
    {\Large Heinrich-Heine-Universität Düsseldorf}\\
    \vspace{0.5em}
    {\large Computer Vision, Computer Graphics and Pattern Recognition}\\
    \vspace{1em}
    July 21, 2015
  \end{center}
  \endgroup
\end{titlepage}

\cleardoublepage

\tableofcontents

\cleardoublepage

\mainmatter
\pagestyle{fancy}

\chapter{Introduction}

In the age of machine learning statistics and random variables have caught the
attention of computer scientists and software engineers. They try to estimate
the variables' parameters and properties from observed data and compute the
distributions of dependent variables from there. In this thesis we tackle the
latter problem. Its complexity varies from instance to instance.

As a simple example you might be interested in the sum of two estimated random
variables, which you assume to be Gaussian distributed. The sum's distribution
can be worked out formally through basic analysis, which can even be done by
hand.
\begin{equation*}
  X \sim \mathcal{N}(\mu, \sigma) \qquad Y \sim \mathcal{N}(\nu, \tau) \qquad \Rightarrow \qquad Z = X + Y \sim \mathcal{N}(\mu + \nu, \sigma + \tau)
\end{equation*}

Most cases however are not manageable without the assistance of computing
software like Maple \cite{maple}. The following Maple program computes the
probability density function (PDF) of the quotient of two random variables as
defined in the previous paragraph. A quick look at figure
\ref{fig:intro-maple-pdf} should tell you, that you probably would not stand a
chance with pen and paper.
\begin{minted}{python}
  with(Statistics);
  X := RandomVariable(Normal(mu, sigma));
  Y := RandomVariable(Normal(nu, tau));
  Z := X/Y;
  PDF(Z, z);
\end{minted}
\begin{figure}
  \begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{thesis/introduction/maple-pdf}
    \caption{Symbolic PDF}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{thesis/introduction/maple-plot}
    \caption{Plot for $\mu = 2, \nu = 0.5, \sigma = \tau = 1$}
  \end{subfigure}
  \caption{Maple finds a symbolic representation of the result's distribution}
  \label{fig:intro-maple-pdf}
\end{figure}

The approach of ramping up computing power and letting the computer find a
symbolic solution also has its limitations though. If you combine enough random
variables with different operations, a technical computing solution will either
be unable to find a solution or the found solution will contain so many
left-over integrals and costly functions, that actually evaluating it is not
feasible. At this point you have to resort to sampling. This means, that you
sample $n$ times from your random variables and compute $n$ samples of the
dependent variable. You can then use sample statistics to estimate basic
properties of the dependent variable and a histogram of the samples is an
approximation to its PDF. The probabilistic programming community \cite{ppl} has
developed various libraries and whole programming languages to explore sampling
techniques. The next program is written in one of these called Church
\cite{church}. It samples from three random variables with different
distributions and computes samples of the dependent variable from them. We
obtain the approximation in figure \ref{fig:church-pdf}, even when Maple was not
able to handle this at all and just reported $\infty$.
\begin{minted}{scheme}
  (define (sample)
    (define x1 (gaussian 1 2))
    (define x2 (gamma 1 3))
    (define y (exponential 2))

    (/ (* x1 x2) y))

  (define samples (repeat 100000 sample))
\end{minted}
\begin{figure}[h]
  \centering
  \includegraphics[width=300px]{thesis/introduction/church-pdf}
  \caption{Approximation to a complicated PDF obtained by sampling}
  \label{fig:church-pdf}
\end{figure}

Symbolics give you a compact representation of exact solutions. The downside is,
that symbolics are restricted to simple problems and the resulting formulas can
be very complicated and expensive to evaluate. Sampling can cope with every
combination, that is computable, but is only approximate. Our approach lies
between symbolics and sampling, a symbolic approximation. We find a symbolic
representation that can approximate arbitrary probability distributions and at
the same time is closed under certain operations. So you can do calculations
with distributions of your choosing and still obtain a symbolic result.

We will
\begin{itemize}
\item formalize the problem and explain our idea in detail (Section
  \ref{ch:idea})
\item describe a way to represent any distribution as a mixture of Gaussians
  (Section \ref{sec:em})
\item prove, that mixture distributions are closed under certain operations and
  are thus a good fit as a mathematical framework (Section \ref{sec:closure})
\item explore the viability of Gaussians as component distributions (Section
  \ref{sec:gaussians})
\item provide a julia implementation (Section \ref{ch:implementation})
\item compare our results to ones that were derived symbolically or through
  sampling (Section \ref{ch:comparisons})
\end{itemize}

\chapter{Our Approach}
\label{ch:idea}

Consider $X_{i} \in \mathbb{R}, i \in \{ 1, \dots, n \}$ $n$ independent random
variables with known PDFs $p(X_{i} = x)$,
$f : \bigtimes_{i} \supp(X_{i}) \rightarrow \mathbb{R}$ a function defined on at
least the support of all the $X_{i}$ and $Y = f(X_{1}, \dots, X_{n})$ the
dependent variable. The task is then to derive the probability distribution of
$Y$ or at least an approximation to it from the known distributions of the
$X_{i}$.

As explained in the first chapter, this problem is very hard in its full
generality. Therefore we are going simplify it by restricting the $X_{i}$ as
well as $f$. First, we restrict the distributions of the $X_{i}$ to mixture
distributions, especially mixtures of Gaussians. The first argument for mixtures
of Gaussians is, that with enough components you can approximate any
distribution to arbitrary precision as you can see in figure
\ref{fig:idea-em}. So we have simplified one aspect of the problem, while still
being able to model any distribution. The other argument is, that Gaussian
distributions are closed under addition and subtraction and the products and
quotients can be closely approximated by mixtures of Gaussians. This is
important, because these closure properties of distributions are preserved, when
you consider a mixture of them, as we see in section \ref{sec:closure}.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{thesis/idea/em}
  \caption{A gamma distribution approximated by mixtures of Gaussians}
  \label{fig:idea-em}
\end{figure}

Secondly, we only consider $f$, that are differentiable and invertible in the
first argument. The reasoning is that for such $f$ there is a lemma called
\emph{multivariate change of variables} \cite[chapter~2.6.2.1]{murphy}, that
lets you reduce the distribution of $Y$ to the joint distribution of the
$X_{i}$. This in turn lets us prove, that the family of mixture distributions is
closed under any such $f$ (section \ref{sec:closure}).

With these restrictions in place we can give symbolic approximations for any $Y$
as long as Gaussian distributions are closed under $f$ or the result of $f$ can
at least be reasonably well approximated by a mixture of Gaussians.

\chapter{The Details}
\label{ch:theory}

In this chapter we will work out the details. We begin with explaining how to
convert arbitrary distributions into mixtures of Gaussians. Then we will prove,
that mixture distributions are closed under certain functions. In the end we
will look at Gaussians as component distributions of mixture distributions and
give exact results and approximations for basic function applications.

\section{Modelling Distributions as Mixtures of Gaussians}
\label{sec:em}

The first step is based on the expectation-maximization (EM) algorithm
\cite[chapter~11.4.2]{murphy}, an iterative algorithm to fit a mixture of
Gaussians to best explain a set of samples. To find a mixture of Gaussians, that
models a given distribution $p$, you first draw a set of samples from $p$. These
samples are obviously distributed according to $p$, so when you fit a mixture of
Gaussians with EM to best explain them, i.e. maximize the likelihood, it will
closely model $p$.

\begin{figure}[h]
  \centering
  \begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{thesis/theory/em-laplace}
    \caption{A Laplace distribution with $a = 1, b = 2$}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{thesis/theory/em-chisq}
    \caption{A chi-squared distribution with $k = 3$}
  \end{subfigure}
  \caption{Some distributions modelled as mixtures of Gaussians}
  \label{fig:theory-em}
\end{figure}

As shown in figure \ref{fig:theory-em} EM also works for distributions, that do
not share many properties with Gaussians, but yields the best results for
distributions that are symmetric and have whole $\mathbb{R}$ as support.

\section{Closure of Mixtures}
\label{sec:closure}

Let $n \in \mathbb{N}_{0}$, $X$ a real random variable with mixture distribution
and $Y_{i}, i \in \{ 1, \dots, n \}$ another $n$ real random variables with
mixture distributions.
\begin{equation*}
  p(X = x) = \sum_{i = 1}^{m} \alpha_{i} \cdot p(X_{i} = x) \qquad p(Y_{i} = y) = \sum_{j = 1}^{m_{i}} \beta_{i,j} \cdot p(Y_{i,j} = y)
\end{equation*}

Consider a function $f : S \times_{i = 1}^{n} S_{i} \rightarrow \mathbb{R}$,
where $S \supseteq \supp(X)$ and
$S_{i} \supseteq \supp(Y_{i}), i \in \{ 1, \dots, n \}$. Then we might ask
ourselves, how $Z = f(X, Y_{1}, \dots, Y_{n})$ is distributed. So we are looking
to evaluate
\begin{equation*}
  p(Z = z) = \int_{S_{n}}\dots\int_{S_{1}} p(Z = 1, Y_{1} = y_{1}, \dots, Y_{n} = y_{n})~\mathrm{d}y_{1}\dots\mathrm{d}y_{n}
\end{equation*}

First, we have to tie the joint PDF of $Z$ and $Y_{1, \dots, n}$ to the joint
PDF of $X$ and $Y_{1, \dots, n}$. Such an expression can be derived using
\emph{multivariate change of variables} \cite[chapter~2.6.2.1]{murphy}, which I
will restate here.

\begin{lemma}[Multivariate change of variables]
  \label{lemma:multivariate-change}
  Let $f : \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$ be differentiable and
  invertible, $X \in \mathbb{R}^{n}$ a random vector and $Y = f(X)$. Then the
  distribution of $Y$ is given by
  \begin{equation*}
    p(Y = y) = \left| \det J \right| \cdot p(X = f^{-1}(y))
  \end{equation*}
  where $J$ is the Jacobian of $f^{-1}$, i.e.
  \begin{equation*}
    J = \frac{\partial X}{\partial Y} = \begin{pmatrix}
      \frac{\partial X_{1}}{\partial Y_{1}} & \cdots & \frac{\partial X_{1}}{\partial Y_{n}}\\
      \vdots & \ddots & \vdots\\
      \frac{\partial X_{n}}{\partial Y_{1}} & \cdots & \frac{\partial X_{n}}{\partial Y_{n}}
    \end{pmatrix}
  \end{equation*}
\end{lemma}

\begin{lemma}
  \label{lemma:joint-pdf}
  If $f$ is differentiable and invertible in the first argument, we can express
  the joint PDF of $Z$ and $Y_{1, \dots, n}$ in terms of the joint PDF of $X$
  and $Y_{1, \dots, n}$ as
  \begin{equation*}
    p(Z = z, Y_{1, \dots, n} = y_{1, \dots, n}) = \left| \frac{\mathrm{d}f_{y_{1}, \dots, y_{n}}^{-1}(z)}{\mathrm{d}z} \right| \cdot p\left(X = f_{y_{1}, \dots, y_{n}}^{-1}(z), Y_{1, \dots, n} = y_{1, \dots, n}\right)
  \end{equation*}
\end{lemma}
\begin{proof2}
  Define $g : S \times_{i = 1}^{n} S_{i} \rightarrow \mathbb{R}^{n + 1}$ as
  \begin{equation*}
    g \begin{pmatrix}
      x\\y_{1}\\\vdots\\y_{n}
    \end{pmatrix} = \begin{pmatrix}
      f(x, y_{1}, \dots, y_{n})\\
      y_{1}\\\vdots\\y_{n}
    \end{pmatrix}
  \end{equation*}
  Then $g$ is invertible and differentiable and the inverse is given by
  \begin{equation*}
    g^{-1}\begin{pmatrix}
      z\\y_{1}\\\vdots\\y_{n}
    \end{pmatrix} = \begin{pmatrix}
      f_{y_{1}, \dots, y_{n}}^{-1}(z)\\
      y_{1}\\\vdots\\y_{n}
    \end{pmatrix}
  \end{equation*}
  Next we examine the Jacobian $J$ of $g^{-1}$. Let
  $(z, y_{1}, \dots, y_{n})^{T} \in \mathbb{R}^{n + 1}$.
  \begin{equation*}
    J :=
    \begin{pmatrix}
      \frac{\mathrm{d}f_{y_{1}, \dots, y_{n}}^{-1}}{\partial z} & \frac{\mathrm{d}f_{y_{1}, \dots, y_{n}}^{-1}}{\partial y_{1}} & \cdots & \frac{\mathrm{d}f_{y_{1}, \dots, y_{n}}^{-1}}{\partial y_{n}}\\
      \frac{\mathrm{d} y_{1}}{\mathrm{d} z} & \frac{\mathrm{d} y_{1}}{\mathrm{d} y_{1}} & \cdots & \frac{\mathrm{d} y_{1}}{\mathrm{d} y_{n}}\\
      \vdots & \vdots & \ddots & \vdots\\
      \frac{\mathrm{d} y_{n}}{\mathrm{d} z} & \frac{\mathrm{d} y_{n}}{\mathrm{d} y_{1}} & \cdots & \frac{\mathrm{d} y_{n}}{\mathrm{d} y_{n}}
    \end{pmatrix}
    =
    \begin{pmatrix}
      \frac{\mathrm{d}f_{y_{1}, \dots, y_{n}}^{-1}}{\partial z} & \frac{\mathrm{d}f_{y_{1}, \dots, y_{n}}^{-1}}{\partial y_{1}} & \cdots & \frac{\mathrm{d}f_{y_{1}, \dots, y_{n}}^{-1}}{\partial y_{n}}\\
      0 & 1 & \cdots & 0\\
      \vdots & \vdots & \ddots & \vdots\\
      0 & 0 & \cdots & 1
    \end{pmatrix}
  \end{equation*}
  $J$ is an upper-triangular matrix, which means
  \begin{equation*}
    \det J = \frac{\mathrm{d}f_{y_{1}, \dots, y_{n}}^{-1}}{\partial z}
  \end{equation*}
  Therefore the claimed equality follows from lemma \ref{lemma:multivariate-change}.
\end{proof2}

Equipped with lemma \ref{lemma:joint-pdf} we can now derive the general form of
$Z$'s distribution.

\begin{lemma}
  \label{lemma:mixture}
  If $f$ is differentiable and invertible in the first argument, the
  distribution of $Z$ is a mixture distribution given by the following equation
  \begin{equation*}
    p(Z = z) = \sum_{i = 1}^{m} \sum_{k_{1} = 1}^{m_{1}} \dots \sum_{k_{n} = 1}^{m_{n}} \alpha_{i} \left( \prod_{j = 1}^{n} \beta_{j,k_{j}} \right) p(f(X_{i}, Y_{1,k_{1}}, \dots, Y_{n,k_{n}}) = z)
  \end{equation*}
\end{lemma}
\begin{proof2}
  Let $x = f_{y_{1}, \dots, y_{n}}^{-1}(z)$.
  \begin{align*}
    & p(Z = z)\\
    \intertext{Expand marginal distribution as an integral over the joint distribution}
    = & \int_{S_{n}} \dots \int_{S_{1}} p(Z = z, Y_{1, \dots, n} = y_{1, \dots, n})~\mathrm{d}y_{1}\dots \mathrm{d}y_{n}\\
    \intertext{Rewrite it using lemma \ref{lemma:joint-pdf}}
    = & \int_{S_{n}} \dots \int_{S_{1}} \left| \frac{\mathrm{d}x}{\mathrm{d}z} \right| \cdot p\left(X = f_{y_{1}, \dots, y_{n}}^{-1}(z), Y_{1, \dots, n} = y_{1, \dots, n}\right)~\mathrm{d}y_{1}\dots \mathrm{d}y_{n}\\
    \intertext{Use the fact, that $X$ and $Y_{i}$ are independent, i.e. that the joint PDF can be factorized}
    = & \int_{S_{n}} \dots \int_{S_{1}} \left| \frac{\mathrm{d}x}{\mathrm{d}z} \right| \cdot p\left(X = f_{y_{1}, \dots, y_{n}}^{-1}(z)\right) \prod_{j = 1}^{n} p\left(Y_{j} = y_{j}\right)~\mathrm{d}y_{1}\dots \mathrm{d}y_{n}\\
    \intertext{Plug in the definitions}
    = & \int_{S_{n}} \dots \int_{S_{1}} \left| \frac{\mathrm{d}x}{\mathrm{d}z} \right| \cdot \left(\sum_{i = 1}^{m} \alpha_{i} \cdot p(X_{i} = x)\right) \prod_{j = 1}^{n} \left(\sum_{k = 1}^{m_{j}} \beta_{j,k} \cdot p(Y_{j,k} = y_{j})\right)~\mathrm{d}y_{1}\dots \mathrm{d}y_{n}\\
    \intertext{Pull out the coefficients}
    = & \sum_{i = 1}^{m} \sum_{k_{1} = 1}^{m_{1}} \dots \sum_{k_{n} = 1}^{m_{n}} \alpha_{i} \left( \prod_{j = 1}^{n} \beta_{j,k_{j}} \right) \int\limits_{S_{n}} \dots \int\limits_{S_{1}} \left| \frac{\mathrm{d}x}{\mathrm{d}z} \right| \cdot p(X_{i} = x) \cdot \prod_{j = 1}^{n} p(Y_{j,k_{j}} = y_{j})~\mathrm{d}y_{1}\dots \mathrm{d}y_{n}\\
    \intertext{Join the probability densities together}
    = & \sum_{i = 1}^{m} \sum_{k_{1} = 1}^{m_{1}} \dots \sum_{k_{n} = 1}^{m_{n}} \alpha_{i} \left( \prod_{j = 1}^{n} \beta_{j,k_{j}} \right) \int\limits_{S_{n}} \dots \int\limits_{S_{1}} \left| \frac{\mathrm{d}x}{\mathrm{d}z} \right| \cdot p\left( \ontopof{X_{i} = x}{Y_{1,k_{1}} = y_{1}, \dots, Y_{n,k_{n}} = y_{n}} \right)~\mathrm{d}y_{1}\dots \mathrm{d}y_{n}\\
    \intertext{Reverse application of lemma \ref{lemma:joint-pdf}}
    = & \sum_{i = 1}^{m} \sum_{k_{1} = 1}^{m_{1}} \dots \sum_{k_{n} = 1}^{m_{n}} \alpha_{i} \left( \prod_{j = 1}^{n} \beta_{j,k_{j}} \right) \int\limits_{S_{n}} \dots \int\limits_{S_{1}} p\left( \ontopof{f(X_{i}, Y_{1,k_{1}}, \dots, Y_{n,k_{n}}) = z}{Y_{1,k_{1}} = y_{1}, \dots, Y_{n,k_{n}} = y_{n}} \right)~\mathrm{d}y_{1}\dots \mathrm{d}y_{n}\\
    \intertext{Marginalize the $Y_{i}$}
    = & \sum_{i = 1}^{m} \sum_{k_{1} = 1}^{m_{1}} \dots \sum_{k_{n} = 1}^{m_{n}} \alpha_{i} \left( \prod_{j = 1}^{n} \beta_{j,k_{j}} \right) p(f(X_{i}, Y_{1,k_{1}}, \dots, Y_{n,k_{n}}) = z)
  \end{align*}
\end{proof2}

\begin{lemma}
  \label{lemma:components}
  If $f(X_{i}, Y_{1,k_{1}}, \dots, Y_{n,k_{n}})$ has a Gaussian mixture
  distribution, $Z$ has a Gaussian mixture distribution as well.
\end{lemma}
\begin{proof2}
  Trivial.
\end{proof2}

Lemma \ref{lemma:mixture} and \ref{lemma:components} together are the foundation
of this thesis. They say, that if $X$ and the $Y_{i}$ have a mixture of $P$
(e.g. Gaussians) distribution, $Z$ will have a mixture of $P$ distribution as
well, if $f$ applied to the component distributions of $X$ and $Y_{i}$ is
distributed as $P$ or a mixture of $P$. This means, that to prove results in our
restricted setting, we only need to prove it for $P$-distributed variables and
it will generalize to mixtures of $P$ automatically.

\section{Gaussians as Component Distributions}
\label{sec:gaussians}

In this section we focus on Gaussian distributions and how to express the
distributions of basic $f(X_{1}, \dots, X_{n})$ as mixtures of gaussians.

\subsection{Affine Transformations}
\label{sec:affine-transforms}

Let $X$ be a random variable with a Gaussian distribution, i.e.
$X \sim \mathcal{N}(\mu, \sigma^{2})$. A basic thing to do with $X$ is to apply
an affine transformation, because every operation involved is just a
transformation of the parameters of the original distribution, as we will
presently see. Along the way we will also reduce subtraction and division to
addition respectively multiplication, because they enhance the usability of the
julia library.

First we need to establish two rearrangement rules for Gaussian distributions.
\begin{lemma}
  \label{lemma:x-mu-symmetry}
  Let $a \in \mathbb{R}$. Then
  $\mathcal{N}(x + a \mid \mu, \sigma^{2}) = \mathcal{N}(x \mid \mu - a,
  \sigma^{2})$.
\end{lemma}
\begin{proof2}
  \begin{align*}
    \mathcal{N}(x + a \mid \mu, \sigma^{2}) = & \frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp\left( - \frac{\left((x + a) - \mu \right)^{2}}{2\sigma^{2}} \right)\\
    = & \frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp\left( - \frac{\left(x - (\mu - a) \right)^{2}}{2\sigma^{2}} \right) = \mathcal{N}(x \mid \mu - a, \sigma^{2})
  \end{align*}
\end{proof2}

\begin{lemma}
  \label{lemma:gaussian-parameter-scaling}
  Let $a \in \mathbb{R} \setminus \{ 0 \}$. Then
  $\mathcal{N}(ax \mid \mu, \sigma^{2}) = \frac{1}{|a|} \cdot \mathcal{N}\left(x \mid
  \frac{\mu}{a}, \frac{\sigma^{2}}{a^{2}}\right)$.
\end{lemma}
\begin{proof2}
  \begin{align*}
    \mathcal{N}(ax \mid \mu, \sigma^{2}) = & \frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp\left( -\frac{\left(ax - \mu \right)^{2}}{2\sigma^{2}} \right)\\
    = & \frac{1}{\sqrt{2 \pi \left( \frac{\sigma a}{a} \right)^{2}}} \exp\left( -\frac{a^{2}\left(x - \frac{\mu}{a} \right)^{2}}{2\sigma^{2}} \right)\\
    = & \frac{1}{|a|} \frac{1}{\sqrt{2 \pi \frac{\sigma^{2}}{a^{2}}}} \exp\left( -\frac{\left(x - \frac{\mu}{a} \right)^{2}}{2 \frac{\sigma^{2}}{a^{2}}} \right)\\
    = & \frac{1}{|a|} \cdot \mathcal{N}\left( x \mid \frac{\mu}{a}, \frac{\sigma^{2}}{a^{2}} \right)
  \end{align*}
\end{proof2}

Next we restate a special case of lemma \ref{lemma:multivariate-change} taken
from \cite[chapter~2.6.2]{murphy}, because it will make it easier to see, that
and how it is used here.
\begin{lemma}[Change of variables]
  \label{lemma:change}
  Let $f : \mathbb{R} \rightarrow \mathbb{R}$ be differentiable, monotonic and
  hence invertible. We can ``unapply'' $f$ through the following formula
  \begin{equation*}
    p(f(X) = y) = \left| \frac{\mathrm{d}f^{-1}}{\mathrm{d}y} \right| \cdot p(X = f^{-1}(y))
  \end{equation*}
\end{lemma}

With these in place we can now examine addition, subtraction, multiplication and
division with a scalar.

\vspace{1em}

\begin{lemma}
  $-X \sim \mathcal{N}(-\mu, \sigma^{2})$
\end{lemma}
\begin{proof2}
  This follows directly from lemma \ref{lemma:gaussian-parameter-scaling} and
  \ref{lemma:change}.
  \begin{equation*}
    p(-X = x) = \mathcal{N}((-1)x \mid \mu, \sigma^{2}) = \frac{1}{|{-1}|} \cdot \mathcal{N}\left( x \mid \frac{\mu}{{-1}}, \frac{\sigma^{2}}{({-1})^{2}} \right) = \mathcal{N}(x \mid {-\mu}, \sigma^{2})
  \end{equation*}
\end{proof2}

\begin{lemma}
  Let $a \in \mathbb{R}$. Then
  $a + X = X + a \sim \mathcal{N}(\mu + a, \sigma^{2})$.
\end{lemma}
\begin{proof2}
  This follows directly from lemma \ref{lemma:x-mu-symmetry} and
  \ref{lemma:change}.
  \begin{align*}
    p(X + a = x) = p(X = x - a) \cdot \left| \frac{\mathrm{d}(x - a)}{\mathrm{d}x} \right| = \mathcal{N}(x - a \mid \mu, \sigma^{2}) = \mathcal{N}(x \mid \mu + a, \sigma^{2})
  \end{align*}
\end{proof2}

\begin{corollary}
  Let $a \in \mathbb{R}$. Then $X - a \sim \mathcal{N}(\mu - a, \sigma^{2})$.
\end{corollary}
\begin{proof2}
  $X - a = X + (-a)$
\end{proof2}

\begin{corollary}
  Let $a \in \mathbb{R}$. Then $a - X \sim \mathcal{N}(a - \mu, \sigma^{2})$.
\end{corollary}
\begin{proof2}
  $a - X = a + (-X)$
\end{proof2}

\begin{lemma}
  Let $a \in \mathbb{R} \setminus \{ 0 \}$. Then
  $aX = Xa \sim \mathcal{N}(a\mu, a^{2}\sigma^{2})$.
\end{lemma}
\begin{proof2}
  This follows from lemma \ref{lemma:gaussian-parameter-scaling} and
  \ref{lemma:change}.
  \begin{align*}
    p(aX = x) & = \left| \frac{\mathrm{d}\frac{x}{a}}{\mathrm{d}x} \right| \cdot p\left( X = \frac{x}{a} \right)\\
              & = \left|\frac{1}{a}\right| \cdot p\left( X = \frac{x}{a} \right)\\
              & = \left|\frac{1}{a}\right| \cdot \mathcal{N}\left(\frac{x}{a} \mid \mu, \sigma^{2}\right)\\
              & = \frac{1}{|a|} \cdot |a| \cdot \mathcal{N}\left(x \mid a\mu, a^{2}\sigma^{2}\right)\\
              & = \mathcal{N}\left(x \mid a\mu, a^{2}\sigma^{2}\right)
  \end{align*}
\end{proof2}

\begin{corollary}
  Let $a \in \mathbb{R} \setminus \{ 0 \}$. Then
  $\frac{X}{a} \sim \mathcal{N}\left( \frac{\mu}{a}, \frac{\sigma^{2}}{a^{2}}
  \right)$.
\end{corollary}
\begin{proof2}
  $\frac{X}{a} = X \cdot \frac{1}{a}$
\end{proof2}

Note, that we did not cover $\frac{a}{X}$. Division is not monotonic for a
variable denominator, so lemma \ref{lemma:change} does not hold and the
distribution of $\frac{a}{X}$ cannot be evaluated with the techniques used in
this section.

\subsection{Affine Transformations of $n$ Variables}

A natural enhancement of section \ref{sec:affine-transforms} is to generalize it
to multiple variables. Therefore we will first determine the distribution of the
sum of two Gaussian distributed variables.

\begin{lemma}
  \label{lemma:gaussian-sum}
  Let $X \sim \mathcal{N}(\mu, \sigma^{2}), Y \sim \mathcal{N}(\nu, \tau^{2})$
  be two independent, Gaussian distributed variables. The distribution of their
  sum is
  \begin{equation*}
    X + Y \sim \mathcal{N}(\mu + \nu, \sigma^{2} + \tau^{2})
  \end{equation*}
\end{lemma}
\begin{proof2}
  For a proof see \cite[Satz~11.9]{krengel}.
\end{proof2}

With lemma \ref{lemma:gaussian-sum} combined with the previous section, we can
now calculate the distribution of a linear combination of two Gaussian
distributed variables. Applying this inductively, we can evaluate affine
transformations of $n$ variables.

\subsection{Product and Quotient of Gaussian Random Variables}

Multiplication and division are indeed monotonic and invertible in the first
argument, but differ from the previous operations in that Gaussian distributions
are not closed under them. Actually the products and quotients of two gaussians
do not have any standard distributions at all. So to stay inside the realm of
mixtures of Gaussians, we have to approximate them with a mixture of Gaussians
using numerical integration algorithms.

Let $X$ and $Y$ be two normally distributed variables
\begin{equation*}
  X \sim \mathcal{N}(\mu, \sigma^{2}) \qquad Y \sim \mathcal{N}(\nu, \tau^{2})
\end{equation*}
and $Z = X \cdot Y$ their product. Then $Z$ is distributed as follows
\begin{align*}
  p(Z = z) & = \int_{-\infty}^{\infty} \frac{1}{|y|} \cdot p\left(X = \frac{z}{y}\right) \cdot p(Y = y)~\mathrm{d}y\\
           & = \int_{-\infty}^{\infty} \frac{1}{|y|} \cdot \mathcal{N}\left( \frac{z}{y} \mid \mu, \sigma^{2} \right) \cdot \mathcal{N}(y \mid \nu, \tau^{2})~\mathrm{d}y\\
           & = \int_{-\infty}^{\infty} \frac{1}{|y|} \cdot |y| \cdot \mathcal{N}\left( z \mid y\mu, y^{2}\sigma^{2} \right) \cdot \mathcal{N}(y \mid \nu, \tau^{2})~\mathrm{d}y\\
           & = \int_{-\infty}^{\infty} \mathcal{N}\left( z \mid y\mu, y^{2}\sigma^{2} \right) \cdot \mathcal{N}(y \mid \nu, \tau^{2})~\mathrm{d}y
\end{align*}

We would like to write this as a mixture of Gaussians, i.e.
$\sum_{i} \alpha_{i} \cdot \mathcal{N}(\mu_{i}, \sigma_{i})$. Gaussian
quadrature comes to mind, a family of numerical integration algorithms, that
approximate
\begin{equation*}
  \int_{a}^{b} f(x) \cdot w(x)~\mathrm{d}x \approx \sum_{i = 1}^{n} w_{i} f(x_{i})
\end{equation*}
for various combinations of $a$, $b$ and $w$ as long as the integrand can be
written as $f(x) \cdot w(x)$, where $w$ is a weight function determined by the
specific algorithm and $f$ an arbitrary function from $C^{2n - 1}[a, b]$, if you
want to approximate with $n$ summands. It probably does not need to be
mentioned, but the approximation becomes more accurate with increasing $n$.

\subsubsection{Gauss-Hermite Quadrature}

Gauss-Hermite quadrature is a special case of Gauss quadrature for integrals of
the form
\begin{equation*}
  \int_{-\infty}^{\infty} f(x) \cdot \exp(-x^{2})~\mathrm{d}x
\end{equation*}
This looks like a perfect fit for our integral, because the PDF of a Gaussian
distribution is practically of the form $a \cdot \exp(-x^{2})$, so one of the
Gaussians can serve as the weight function, while the other persists as part of
$f$ creates the Gaussians in the resulting mixture of Gaussians.

\begin{align*}
  p(Z = z) & = \int_{-\infty}^{\infty} \mathcal{N}\left( z \mid y\mu, y^{2}\sigma^{2} \right) \cdot \mathcal{N}(y \mid \nu, \tau^{2})~\mathrm{d}y\\
           & = \int_{-\infty}^{\infty} \mathcal{N}\left( z \mid y\mu, y^{2}\sigma^{2} \right) \cdot \frac{1}{\sqrt{2\pi\tau^{2}}} \exp\left( -\frac{(y - \nu)^{2}}{2\tau^{2}} \right)~\mathrm{d}y\\
  \intertext{To transform the integrand into the form $f(x) \cdot \exp(-x^{2})$ we will now substitute
\begin{equation*}
  x = \frac{y - \nu}{\sqrt{2}\tau} \Leftrightarrow y = \sqrt{2}\tau x + \nu \qquad \frac{\mathrm{d}x}{\mathrm{d}y} = \frac{1}{\sqrt{2}\tau}
\end{equation*}}
           & = \frac{1}{\sqrt{\pi}} \int_{-\infty}^{\infty} \mathcal{N}\left( z \mid \left(\sqrt{2}\tau x + \nu\right)\mu, \left(\sqrt{2}\tau x + \nu\right)^{2}\sigma^{2} \right) \cdot \exp\left( -x^{2} \right)~\mathrm{d}x
\end{align*}

With the integrand in the right form we can apply the approximation.
\begin{equation*}
  p(Z = z) \approx \sum_{i = 1}^{n} \frac{w_{i}}{\sqrt{\pi}} \cdot \mathcal{N}\left( z \mid \left(\sqrt{2 \tau^{2}} x_{i} + \nu\right)\mu, \left(\sqrt{2 \tau^{2}} x_{i} + \nu\right)^{2}\sigma^{2} \right)
\end{equation*}
where $x_{i}$ and $w_{i}$ are the positions and weights as defined by the
Gauss-Hermite quadrature method. Interestingly it follows from the construction
of the method, that $\sum w_{i} = \sqrt{\pi}$, so the approximation is a convex
combination and thus a real mixture distribution.

\begin{figure}[h]
  \centering
  \begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{thesis/theory/gauss-hermite}
    \caption{An overview of the general fit}
    \label{fig:gauss-laguerre}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{thesis/theory/gauss-hermite-zoomed}
    \caption{Zoomed in around $0$}
    \label{fig:gauss-laguerre-zoomed}
  \end{subfigure}
  \caption{A product distribution approximated by a mixture of Gaussians}
\end{figure}
A plot shows, that it is a reasonable fit. In figure \ref{fig:gauss-laguerre} we
see, that the calculated mixture of Gaussians closely follows the true PDF
obtained by sampling except in a neighborhood of $0$. The zoomed-in plot in
figure \ref{fig:gauss-laguerre-zoomed} confirms indeed, that the approximation
still has a similar shape, but is more wobbly than the true PDF in the proximity
of $0$.

The bad fit around $0$ is actually inherent to our problem and choice of
algorithm. Remember the requirement of any Gauss quadrature method, that $f$ be
in $C^{2n - 1}[a, b]$, and then look at the original integral and its integrand
as a function of $y$.
\begin{equation*}
  \int_{-\infty}^{\infty} \mathcal{N}\left( z \mid y\mu, y^{2}\sigma^{2} \right) \cdot \mathcal{N}(y \mid \nu, \tau^{2})~\mathrm{d}y
\end{equation*}
If $z = 0$ and $y$ approaches $0$, the integrand tends towards $\infty$ from
both sides, so it is not even once continuously differentiable in $0$, though it
is continuously differentiable infinite times everywhere else. So any Gauss
quadrature method will be numerically unstable. It is, however, sufficiently
stable for our cause, if you evaluate it with $z$ not too close $0$, which is
also evident in figure \ref{fig:gauss-laguerre}.

The problem obviously persists after the substitution, but now we can pinpoint
exactly, where the instabily will occur. Recall the transformed integral
\begin{equation*}
  \int_{-\infty}^{\infty} \mathcal{N}\left( z \mid \left(\sqrt{2}\tau x + \nu\right)\mu, \left(\sqrt{2}\tau x + \nu\right)^{2}\sigma^{2} \right) \cdot \exp\left( -x^{2} \right)~\mathrm{d}x
\end{equation*}
We have the same problem as before, when $\sqrt{2}\tau x + \nu$ is small,
i.e. $x \approx -\frac{\nu}{\sqrt{2}\tau}$, and
$z \approx \left(\sqrt{2}\tau x + \nu\right)\mu$. In figure
\ref{fig:gauss-hermite-instabilities} we plot the 10 components with minimal
variance. Their $x$-value is their mean and their $y$-value their variance. We
observe, that all of them are in direct proximity of the instability.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\textwidth]{thesis/theory/gauss-hermite-instabilities}
  \caption{The components with minimal variance are in direct proximity of the
    instability}
  \label{fig:gauss-hermite-instabilities}
\end{figure}

\subsubsection{Gauss-Laguerre Quadrature}

Another algorithm in the family of Gauss quadratures is Gauss-Laguerre
quadrature. It is made for integrals of the form
\begin{equation*}
  \int_{0}^{\infty} f(x) \cdot \exp(-x)~\mathrm{d}x
\end{equation*}
which does not match our situation quite as well as Gauss-Hermite does, so we
will need to do some more preparatory work. The motivation for trying
Gauss-Laguerre quadrature is, that this procedure may cope better with the
instability around $0$.

To make Gauss-Laguerre quadrature applicable, we have to rewrite the integral
from $-\infty$ to $\infty$ as two integrals from $0$ to $\infty$.
\begin{align*}
  p(Z = z) & = \int_{-\infty}^{\infty} \mathcal{N}\left( z \mid y\mu, y^{2}\sigma^{2} \right) \cdot \mathcal{N}(y \mid \nu, \tau^{2})~\mathrm{d}y\\
           & = \int_{-\infty}^{0} \mathcal{N}\left( z \mid y\mu, y^{2}\sigma^{2} \right) \cdot \mathcal{N}(y \mid \nu, \tau^{2})~\mathrm{d}y + \int_{0}^{\infty} \mathcal{N}\left( z \mid y\mu, y^{2}\sigma^{2} \right) \cdot \mathcal{N}(y \mid \nu, \tau^{2})~\mathrm{d}y\\
           & = -\int_{\infty}^{0} \mathcal{N}\left( z \mid -y\mu, y^{2}\sigma^{2} \right) \cdot \mathcal{N}(-y \mid \nu, \tau^{2})~\mathrm{d}y + \int_{0}^{\infty} \mathcal{N}\left( z \mid y\mu, y^{2}\sigma^{2} \right) \cdot \mathcal{N}(y \mid \nu, \tau^{2})~\mathrm{d}y\\
           & = \int_{0}^{\infty} \mathcal{N}\left( z \mid -y\mu, y^{2}\sigma^{2} \right) \cdot \mathcal{N}(-y \mid \nu, \tau^{2})~\mathrm{d}y + \int_{0}^{\infty} \mathcal{N}\left( z \mid y\mu, y^{2}\sigma^{2} \right) \cdot \mathcal{N}(y \mid \nu, \tau^{2})~\mathrm{d}y
\end{align*}

Both of these integrals can be solved approximately through Gauss-Laguerre
integration. Yet we will first have to rewrite them in the required form and
once again the integrands are not sufficiently differentiable in $0$. This time
we may be able to avert the problem though through a very non-mathematical
approach. Instead of integrating over $[0, \infty]$ we integrate over
$[\varepsilon, \infty]$ for an $\varepsilon > 0$. For some $\varepsilon$ the
payoff from not evaluating the integrand in the proximity of the instability may
be greater than the reduced accuracy from ignoring an $\varepsilon$-strip of the
integral.

We begin with rearranging the first integrand to the form $f(y) * e^{-y}$.
\begin{align*}
  & \int_{\varepsilon}^{\infty} \mathcal{N}\left( z \mid -y\mu, y^{2}\sigma^{2} \right) \cdot \mathcal{N}(-y \mid \nu, \tau^{2})~\mathrm{d}y \\
  =& \int_{\varepsilon}^{\infty} \mathcal{N}\left( z \mid -y\mu, y^{2}\sigma^{2} \right) \cdot \frac{1}{\sqrt{2 \pi \tau^{2}}} \exp\left( -\frac{(-y - \nu)^{2}}{2\tau^{2}} \right)~\mathrm{d}y\\
  =& \int_{\varepsilon}^{\infty} \mathcal{N}\left( z \mid -y\mu, y^{2}\sigma^{2} \right) \cdot \frac{1}{\sqrt{2 \pi \tau^{2}}} \exp\left( -\left( \frac{y + \nu}{\sqrt{2\tau^{2}}} \right)^{2} \right)~\mathrm{d}y\\
  \intertext{Substitute $t = \left( \frac{y + \nu}{\sqrt{2\tau^{2}}} \right)^{2}$, $y = \pm \sqrt{2\tau^{2} t} - \nu$, $\frac{\mathrm{d}t}{\mathrm{d}y} = \frac{y + \nu}{\tau^{2}} = \frac{(\pm \sqrt{2\tau^{2} t} - \nu) + \nu}{\tau^{2}} = \pm \frac{\sqrt{2t}}{\sqrt{\tau^{2}}}$}
  =& \int_{\frac{(\varepsilon + \nu)^{2}}{2\tau^{2}}}^{\infty} \mathcal{N}\left( z \mid -\left( \sqrt{2\tau^{2} t} - \nu \right)\mu, \left( \sqrt{2\tau^{2} t} - \nu \right)^{2}\sigma^{2} \right) \cdot \frac{1}{2 \sqrt{\pi t}} \exp\left( -t \right)~\mathrm{d}t\\
  \intertext{Substitute $s = t - \frac{(\varepsilon + \nu)^{2}}{2\tau^{2}}$, $t = s + \frac{(\varepsilon + \nu)^{2}}{2\tau^{2}}$, $\frac{\mathrm{d}s}{\mathrm{d}t} = 1$}
  =& \int_{0}^{\infty} \mathcal{N}\left( z \left| \ontopof{-\left( \sqrt{2\tau^{2}s + (\varepsilon + \nu)^{2}} - \nu \right)\mu}{\left( \sqrt{2\tau^{2}s + (\varepsilon + \nu)^{2}} - \nu \right)^{2}\sigma^{2}} \right.\right) \cdot \frac{\exp\left( -\frac{(\varepsilon + \nu)^{2}}{2\tau^{2}} \right)}{2 \sqrt{\pi \left( s + \frac{(\varepsilon + \nu)^{2}}{2\tau^{2}} \right)}} \exp(-s)~\mathrm{d}s
\end{align*}

Next we rearrange the second integrand.
\begin{align*}
  & \int_{\varepsilon}^{\infty} \mathcal{N}\left( z \mid y\mu, y^{2}\sigma^{2} \right) \cdot \mathcal{N}(y \mid \nu, \tau^{2})~\mathrm{d}y\\
  =& \int_{\varepsilon}^{\infty} \mathcal{N}\left( z \mid y\mu, y^{2}\sigma^{2} \right) \cdot \frac{1}{\sqrt{2 \pi \tau^{2}}} \exp\left( -\frac{(y - \nu)^{2}}{2\tau^{2}} \right)~\mathrm{d}y\\
  =& \int_{\varepsilon}^{\infty} \mathcal{N}\left( z \mid y\mu, y^{2}\sigma^{2} \right) \cdot \frac{1}{\sqrt{2 \pi \tau^{2}}} \exp\left( -\left( \frac{y - \nu}{\sqrt{2\tau^{2}}} \right)^{2} \right)~\mathrm{d}y\\
  \intertext{Substitute $t = \left( \frac{y - \nu}{\sqrt{2\tau^{2}}} \right)^{2}$, $y = \pm \sqrt{2\tau^{2} t} + \nu$, $\frac{\mathrm{d}t}{\mathrm{d}y} = \frac{y - \nu}{\tau^{2}} = \frac{(\pm \sqrt{2\tau^{2} t} + \nu) - \nu}{\tau^{2}} = \pm \frac{\sqrt{2t}}{\sqrt{\tau^{2}}}$}
  =& \int_{\frac{(\varepsilon - \nu)^{2}}{2\tau^{2}}}^{\infty} \mathcal{N}\left( z \mid \left( \sqrt{2\tau^{2} t} + \nu \right)\mu, \left( \sqrt{2\tau^{2} t} + \nu \right)^{2}\sigma^{2} \right) \cdot \frac{1}{2\sqrt{\pi t}} \exp\left( -t \right)~\mathrm{d}t\\
  \intertext{Substitute $s = t - \frac{(\varepsilon - \nu)^{2}}{2\tau^{2}}$, $t = s + \frac{(\varepsilon - \nu)^{2}}{2\tau^{2}}$, $\frac{\mathrm{d}s}{\mathrm{d}t} = 1$}
  =& \int_{0}^{\infty} \mathcal{N}\left( z \left| \ontopof{\left( \sqrt{2\tau^{2} s + (\varepsilon - \nu)^{2}} + \nu \right)\mu}{\left( \sqrt{2\tau^{2} s + (\varepsilon - \nu)^{2}} + \nu \right)^{2}\sigma^{2}} \right.\right) \cdot \frac{\exp\left( -\frac{(\varepsilon - \nu)^{2}}{2\tau^{2}} \right)}{2\sqrt{\pi \left( s + \frac{(\varepsilon - \nu)^{2}}{2\tau^{2}} \right)}} \exp(-s)~\mathrm{d}s
\end{align*}

Now we can plug in the two rearrangements and get the approximation as a mixture
of Gaussians by applying the Gauss-Laguerre quadrature.
\begin{align*}
  p(Z = z) \approx& \int_{0}^{\infty} \mathcal{N}\left( z \left| \ontopof{-\left( \sqrt{2\tau^{2}s + (\varepsilon + \nu)^{2}} - \nu \right)\mu}{\left( \sqrt{2\tau^{2}s + (\varepsilon + \nu)^{2}} - \nu \right)^{2}\sigma^{2}} \right.\right) \cdot \frac{\exp\left( -\frac{(\varepsilon + \nu)^{2}}{2\tau^{2}} \right)}{2 \sqrt{\pi \left( s + \frac{(\varepsilon + \nu)^{2}}{2\tau^{2}} \right)}} \exp(-s)~\mathrm{d}s\\
                  & + \int_{0}^{\infty} \mathcal{N}\left( z \left| \ontopof{\left( \sqrt{2\tau^{2} s + (\varepsilon - \nu)^{2}} + \nu \right)\mu}{\left( \sqrt{2\tau^{2} s + (\varepsilon - \nu)^{2}} + \nu \right)^{2}\sigma^{2}} \right.\right) \cdot \frac{\exp\left( -\frac{(\varepsilon - \nu)^{2}}{2\tau^{2}} \right)}{2\sqrt{\pi \left( s + \frac{(\varepsilon - \nu)^{2}}{2\tau^{2}} \right)}} \exp(-s)~\mathrm{d}s\\
  \approx& \sum_{i = 1}^{n} w_{i} \frac{\exp\left( -\frac{(\varepsilon + \nu)^{2}}{2\tau^{2}} \right)}{2 \sqrt{\pi \left( x_{i} + \frac{(\varepsilon + \nu)^{2}}{2\tau^{2}} \right)}} \cdot \mathcal{N}\left( z \left|
           \ontopof{-\left( \sqrt{2\tau^{2}x_{i} + (\varepsilon + \nu)^{2}} - \nu \right)\mu}{\left( \sqrt{2\tau^{2}x_{i} + (\varepsilon + \nu)^{2}} - \nu \right)^{2}\sigma^{2}}
           \right.\right)\\
                  &+ \sum_{i = 1}^{n} w_{i} \frac{\exp\left( -\frac{(\varepsilon - \nu)^{2}}{2\tau^{2}} \right)}{2\sqrt{\pi \left( x_{i} + \frac{(\varepsilon - \nu)^{2}}{2\tau^{2}} \right)}} \cdot \mathcal{N}\left( z \left|
                    \ontopof{\left( \sqrt{2\tau^{2} x_{i} + (\varepsilon - \nu)^{2}} + \nu \right)\mu}{\left( \sqrt{2\tau^{2} x_{i} + (\varepsilon - \nu)^{2}} + \nu \right)^{2}\sigma^{2}}
                    \right.\right)
\end{align*}
where the $x_{i}$ are the roots of the $n$-th Laguerre polynomial and the
$w_{i}$ the associated weights.

This time things did not turn out as nice as with Gauss-Hermite quadrature. The
$\sum_{i} w_{i}$ is $1$ and the value of the coefficients is overall not
constant, so the sums are not a real mixture distribution and the weights have
to be normalized. This method also cannot keep up in a direct comparison (figure
\ref{fig:hermite-laguerre}). Both methods find the correct mean and
Gauss-Laguerre quadrature also produces a shape similar to the true probability
density, but apart from that it is pretty far off. The variance in the proximity
of the instability is too small, but overall it is too high. In this example the
sample variance is roughly $40$, closely matched by the Gauss-Hermite
approximation, but the variance of the Gauss-Laguerre mixture is $80$. There may
be a bug in our program or an error in the math of this section regarding the
variance, though we did not find any.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.65\textwidth]{thesis/theory/hermite-laguerre}
  \caption{Gauss-Laguerre cannot keep up with Gauss-Hermite}
  \label{fig:hermite-laguerre}
\end{figure}

\section{Division}

Let $X$ and $Y$ be two normally distributed variables
\begin{equation*}
  X \sim \mathcal{N}(\mu, \sigma^{2}) \qquad Y \sim \mathcal{N}(\nu, \tau^{2})
\end{equation*}
and $Z = \frac{X}{Y}$ their quotient. Then $Z$ is distributed as follows
\begin{align*}
  p(Z = z) & = \int_{-\infty}^{\infty} |y| \cdot p\left(X = zy\right) \cdot p(Y = y)~\mathrm{d}y\\
           & = \int_{-\infty}^{\infty} |y| \cdot \mathcal{N}\left( zy \mid \mu, \sigma^{2} \right) \cdot \mathcal{N}(y \mid \nu, \tau^{2})~\mathrm{d}y\\
           & = \int_{-\infty}^{\infty} |y| \cdot \frac{1}{|y|} \cdot \mathcal{N}\left( z \mid \frac{\mu}{y}, \frac{\sigma^{2}}{y^{2}} \right) \cdot \mathcal{N}(y \mid \nu, \tau^{2})~\mathrm{d}y\\
           & = \int_{-\infty}^{\infty} \mathcal{N}\left( z \mid \frac{\mu}{y}, \frac{\sigma^{2}}{y^{2}} \right) \cdot \mathcal{N}(y \mid \nu, \tau^{2})~\mathrm{d}y
\end{align*}
Now we can apply the same substitution as for multiplication to bring it into
Gauss-Hermite quadrature form and get
\begin{equation*}
  p(Z = z) \approx \sum_{i = 1}^{n} \frac{w_{i}}{\sqrt{\pi}} \cdot \mathcal{N}\left( z \mid \frac{\mu}{\sqrt{2 \tau^{2}} x_{i} + \nu}, \frac{\sigma^{2}}{\left(\sqrt{2 \tau^{2}} x_{i} + \nu\right)^{2}} \right)
\end{equation*}

\chapter{Implementation}
\label{ch:implementation}

\chapter{Comparisons}
\label{ch:comparisons}

\begin{appendices}
  \chapter{Source Code}
  \label{ch:source}

  \inputminted{julia}{src/Transforms.jl}
\end{appendices}

\printbibliography

\pagestyle{empty}
\cleardoublepage

\vspace*{10em}
{\LARGE Ehrenwörtliche Erklärung}
\vspace{1em}

Hiermit versichere ich, dass ich die Bachelorarbeit selbstständig und ohne
Benutzung anderer als der angegebenen Hilfsmittel verfasst und alle Stellen, die
wörtlich oder sinngemäß aus veröffentlichten Schriften entnommen sind, als
solche kenntlich gemacht habe.

\vspace{5em}

\begin{minipage}{0.4\textwidth}
  \begin{flushleft}
    Düsseldorf, 15. Juli 2015
  \end{flushleft}
\end{minipage}
\hfill
\begin{minipage}{0.4\textwidth}
  \begin{flushright}
    Marten Lienen
  \end{flushright}
\end{minipage}

\end{document}